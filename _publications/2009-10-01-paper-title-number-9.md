<!-- ---
title: "EnVisionVR: A Scene Interpretation Tool for Visual Accessibility Design in Virtual Reality"
collection: publications
category: conferences
permalink: /publication/envisionvr-vr-visual-accessibility
excerpt: 'This paper presents EnVisionVR, an accessibility tool to assist VR scene interpretation with vision-language models. An evaluation with 12 BLV users demonstrated that EnVisionVR significantly improved their ability to locate virtual objects, effectively supporting scene understanding and object interaction.'
# date: 2024-10-28
venue: '(Under Review)'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
# paperurl: 'https://arxiv.org/abs/2410.21091'
# citation: 'Chen, J., Grubert, J., & Kristensson, P. O. (2024). Large Language Model-assisted Speech and Pointing Benefits Multiple 3D Object Selection in Virtual Reality. arXiv preprint arXiv:2410.21091.'
---

Designing visual accessibility systems for virtual reality (VR) is challenging due to the complexity of immersive 3D environments and the need for techniques that can be easily retrofitted into existing applications. While prior work has studied how to enhance visual information or convert it to audio or haptic information, the advancement of vision language models (VLMs) brings about an exciting opportunity to enhance the scene interpretation capability of current visual accessibility systems. In a formative study, we found that blind and low vision (BLV) users faced accessibility barriers such as the lack of screen reader features when using VR technology. In response to this, we developed EnVisionVR, an accessibility tool to assist scene interpretation in VR for BLV users. An empirical study with 12 BLV users demonstrated that EnVisionVR significantly improved their ability to locate virtual objects, and resulted in improved performance in scene understanding and object interaction. -->